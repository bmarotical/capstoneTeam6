{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13183fdd",
   "metadata": {},
   "source": [
    "## Overview ##\n",
    "\n",
    "1. 19 columns were identified as having NULL values, out of which 4 columns have more than 70 percent were NULL, which were dropped\n",
    "2. The following attributes were chosen based on our initial correlation analysis; which are some of the common attributest between test datasets \n",
    "   LotArea , BldgType,  HouseStyle ,  YearBuilt,    FullBath,   HalfBath ,   BedroomAbvGr\n",
    "3. Another correlation analysis was performed among the selected attributes to avoid multicollinearity \n",
    "4. In order to avoid outliers, LotArea greater than 50,000 sq.ft. were eliminated \n",
    "5. Test datasets are gathered from Delaware- Bear, Delaware- Newark, Delaware-Wilmington and the latest data from Iowa- Ames.\n",
    "6. Full Bath, Half Bath, Year Built and lot area are the most significant predictors in the model\n",
    "7. With this prediction model, predicted house price  is off by an average of  $53,600\n",
    "8. The R^2 statistic shows how well the model explains SalePrice.\n",
    "9. In this model, since R^2 and Adjusted R^2 are close, model is not overfit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e31281",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "import seaborn as sns\n",
    "from sklearn import metrics\n",
    "\n",
    "import cpi\n",
    "cpi.update()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0036b553",
   "metadata": {},
   "source": [
    "### Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c385434b",
   "metadata": {},
   "outputs": [],
   "source": [
    "housingData=pd.read_csv('Data/housing.csv')\n",
    "housingData.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716cde05",
   "metadata": {},
   "source": [
    "## Basic EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851d46bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "housingData.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c60042d",
   "metadata": {},
   "outputs": [],
   "source": [
    "housingData.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8877cf4",
   "metadata": {},
   "source": [
    "### Attribute Correlation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7c33e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = housingData.corr()\n",
    "corr.style.background_gradient()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5c1dfa",
   "metadata": {},
   "source": [
    "### FInd missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd45a523",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_missing_percent(data):\n",
    "    \"\"\"\n",
    "    Returns dataframe containing the total missing values and percentage of total\n",
    "    missing values of a column.\n",
    "    \"\"\"\n",
    "    miss_df = pd.DataFrame({'ColumnName':[],'TotalMissingVals':[],'PercentMissing':[]})\n",
    "    for col in data.columns:\n",
    "        sum_miss_val = data[col].isnull().sum()\n",
    "        percent_miss_val = round((sum_miss_val/data.shape[0])*100,2)\n",
    "        miss_df = miss_df.append(dict(zip(miss_df.columns,[col,sum_miss_val,percent_miss_val])),ignore_index=True)\n",
    "    return miss_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be587bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "miss_df = find_missing_percent(housingData)\n",
    "'''Displays columns with missing values'''\n",
    "display(miss_df[miss_df['PercentMissing']>0.0])\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Number of columns with missing values:\"+(str(miss_df[miss_df['PercentMissing']>0.0].shape[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44879cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols = miss_df[miss_df['PercentMissing'] >70.0].ColumnName.tolist()\n",
    "\n",
    "print(\"Number of columns with more than 70%:\"+ str(len(drop_cols)))\n",
    "housingData = housingData.drop(drop_cols,axis=1)\n",
    "#test = test.drop(drop_cols,axis =1)\n",
    "\n",
    "miss_df = miss_df[miss_df['ColumnName'].isin(housingData.columns)]\n",
    "'''Columns to Impute'''\n",
    "impute_cols = miss_df[miss_df['TotalMissingVals']>0.0].ColumnName.tolist()\n",
    "miss_df[miss_df['TotalMissingVals']>0.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc4ad64",
   "metadata": {},
   "source": [
    "### Basic Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d51624",
   "metadata": {},
   "outputs": [],
   "source": [
    "housingData.describe().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5df344",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq1, bin_edges1=np.histogram(housingData.LotArea, bins='fd')\n",
    "\n",
    "plt.figure(figsize=(15,5))  \n",
    "ax1 = plt.subplot(1,2,1)\n",
    "ax1.set_title('LotArea',fontsize=20)\n",
    "ax1.set_xlabel('LotArea',fontsize=20)\n",
    "ax1.set_ylabel('Frequency',fontsize=20)\n",
    "housingData[['LotArea']].hist(bins=bin_edges1,ax = ax1, xlabelsize=10, ylabelsize=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33060c8",
   "metadata": {},
   "source": [
    "### Removing Outlier\n",
    "\n",
    "In order to avoid outliers, LotArea greater than 50,000 sq.ft. were eliminated "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef51bad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping lotArea greater than 50000 to remove outlier \n",
    "housingData = housingData[housingData.LotArea <= 50000].copy()\n",
    "housingData.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4343e4",
   "metadata": {},
   "source": [
    "### Adjust for inflation using CPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235a782c",
   "metadata": {},
   "outputs": [],
   "source": [
    "housingData['ADJUSTED_SalesPrice'] = housingData.apply(lambda x: cpi.inflate(x.SalePrice, x.YrSold), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1fafe5",
   "metadata": {},
   "source": [
    "##### Prepare the data by separating X and y\n",
    "##### Dropping unimportant features, such as <>\n",
    "##### Note that interesting features might be engieered from the dropped features above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551a9094",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=housingData[['LotArea','BldgType','HouseStyle','YearBuilt','FullBath','HalfBath','BedroomAbvGr']].copy()\n",
    "Y=housingData[['ADJUSTED_SalesPrice']]\n",
    "X.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c104af9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.hist(figsize=(15,15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54cc0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y.hist(figsize=(15,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2062bf0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(X[['LotArea', 'YearBuilt', 'FullBath', 'HalfBath',\n",
    "       'BedroomAbvGr']], height=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7132fc59",
   "metadata": {},
   "source": [
    "### Split the data into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09fc2f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into a training set and a test set. \n",
    "# Any number for the random_state is fine, see 42: https://en.wikipedia.org/wiki/42_(number) \n",
    "# We choose to use 20% (test_size=0.2) of the data set as the test set.\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323470cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features =['LotArea', 'YearBuilt', 'FullBath', 'HalfBath','BedroomAbvGr']\n",
    "cat_features = ['BldgType','HouseStyle']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc91d0c",
   "metadata": {},
   "source": [
    "## Data pre-processing\n",
    "We will build a pipeline to do some of the following tasks:\n",
    "\n",
    "- Missing data\n",
    "- Feature scaling (important for certain model such as Gradient Descent based models)\n",
    "- Categorical feature encoding\n",
    "- Outlier removal\n",
    "- Transformation\n",
    "- Custom processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8863dcd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# any missing values?\n",
    "X_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8afb75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# Create the preprocessing pipeline for numerical features\n",
    "# Pipeline(steps=[(name1, transform1), (name2, transform2), ...]) \n",
    "# NOTE the step names can be arbitrary\n",
    "\n",
    "# Step 1 is feature scaling via standardization - making features look like normal-distributed \n",
    "# see sandardization: https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)\n",
    "num_pipeline = Pipeline(\n",
    "    steps=[\n",
    "        #('poly',PolynomialFeatures(degree =2)),  # we will tune differet strategies later\n",
    "        ('scaler', StandardScaler())\n",
    "        ]\n",
    ")\n",
    "\n",
    "# Create the preprocessing pipelines for the categorical features\n",
    "# There are two steps in this pipeline:\n",
    "# Step 1: one hot encoding\n",
    "\n",
    "cat_pipeline = Pipeline(\n",
    "    steps=[\n",
    "                ('onehot', OneHotEncoder())\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Assign features to the pipelines and Combine two pipelines to form the preprocessor\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num_pipeline', num_pipeline, num_features),\n",
    "        ('cat_pipeline', cat_pipeline, cat_features),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19c3e75",
   "metadata": {},
   "source": [
    "## Model traning, tuning, evaluation and selection\n",
    "\n",
    "Next, we attach three different models (Linear, Ridge, XGBoost) to the same pre-processing pipeline and tune the some parameters using GridSearch with cross validation. Then, we compare their performance and choose the best model to proceed. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1dc2e91",
   "metadata": {},
   "source": [
    "### Using Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d98cde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we show how to use GridSearch with K-fold cross validation (K=10) to fine tune the model\n",
    "# we use the accuracy as the scoring metric with training score return_train_score=True\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# try Linear Regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# rf pipeline\n",
    "pipeline_lr = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LinearRegression()),\n",
    "])\n",
    "\n",
    "parameters_lr=[\n",
    "    {\n",
    "        'classifier__fit_intercept': [True,False],\n",
    "        'classifier__copy_X': [True, False],\n",
    "        'classifier__normalize': [True, False]\n",
    "    }\n",
    "]                 \n",
    "\n",
    "grid_search_lr = GridSearchCV(pipeline_lr,parameters_lr, cv=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49bb405d",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50095a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the best performing parameter combination\n",
    "grid_search_lr.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2b67a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build-in CV results keys\n",
    "sorted(grid_search_lr.cv_results_.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad3d4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best linear regression model test score\n",
    "grid_search_lr.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2af5a55",
   "metadata": {},
   "source": [
    "### Using Ridge Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa9d1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# rf pipeline\n",
    "pipeline_rg = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('clf_RG', Ridge()),\n",
    "])\n",
    "\n",
    "parameters_rg=[\n",
    "    {\n",
    "        'clf_RG__alpha': [0,0.2,0.01,1.0],\n",
    "        'clf_RG__copy_X': [True, False],\n",
    "        'clf_RG__fit_intercept': [True, False]\n",
    "    }\n",
    "]                 \n",
    "\n",
    "grid_search_rg = GridSearchCV(pipeline_rg,parameters_rg, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5fe0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_rg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8f35b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best linear regression model test score\n",
    "grid_search_rg.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa64cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best test score\n",
    "print('best linear regression score is: ', grid_search_lr.best_score_)\n",
    "print('best Ridge classifier score is: ', grid_search_rg.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c2f05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the best model\n",
    "# the best parameters are shown, note SimpleImputer() implies that mean strategry is used\n",
    "best_model = grid_search_lr.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7db4fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1a08f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final test on the testing set\n",
    "# To predict on new data: simply calling the predict method \n",
    "# the full pipeline steps will be applied to the testing set followed by the prediction\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# calculate accuracy, Note: y_test is the ground truth for the tesing set\n",
    "# we have similiar score for the testing set as the cross validation score - good\n",
    "\n",
    "#print('Accuracy Score :' (accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8213c332",
   "metadata": {},
   "outputs": [],
   "source": [
    "#===========   R-square and other metrics ===================\n",
    "r_square= metrics.r2_score(y_test, y_pred)\n",
    "mae_y = metrics.mean_absolute_error(y_test, y_pred)\n",
    "mse_y = metrics.mean_squared_error(y_test, y_pred)\n",
    "rmse_y = np.sqrt(metrics.mean_squared_error(y_test, y_pred))\n",
    "\n",
    "print(\"Linear::r_square={0}::mean_absolute_error={1}::mean_square_error={2}::sqrt_mean_square_error={3}::\".format(r_square,mae_y,mse_y,rmse_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8ea98e",
   "metadata": {},
   "source": [
    "## Feature Importance\n",
    "\n",
    "Given that we are using pipeline and one-hot encoding, the feature importance scores are not very straightforward to get. The following code shows how to get the feature importance scores from the Linear regression and create a plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95541b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.named_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a316549",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.named_steps['preprocessor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8868074d",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = best_model.named_steps['classifier'].coef_\n",
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cfa2203",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model['preprocessor'].transformers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b468953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get columnTransformer\n",
    "best_model[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faceb142",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model[0].transformers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2dc02ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_original_feature_names = best_model[0].transformers_[0][2]\n",
    "num_original_feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4901a4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_original_feature_names = best_model[0].transformers_[1][2]\n",
    "cat_original_feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d00ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_new_feature_names = list(best_model[0].transformers_[1][1]['onehot'].get_feature_names(cat_original_feature_names))\n",
    "cat_new_feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886655c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = num_original_feature_names + cat_new_feature_names\n",
    "feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92ec106",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = pd.DataFrame(i, index=feature_names, columns=['importance'])\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4089c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "r.sort_values('importance', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c718c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "r.sort_values('importance', ascending=False).plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf85586c",
   "metadata": {},
   "source": [
    "## Remove unimportant Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c471a41f",
   "metadata": {},
   "source": [
    "## Persists the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8720c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model as a pickle file\n",
    "import joblib\n",
    "joblib.dump(best_model, \"Housing.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2275642",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model from a pickle file\n",
    "saved_linear_clf = joblib.load(\"Housing.pickle\")\n",
    "saved_linear_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11f2d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "housePrice = pd.DataFrame(\n",
    "    {      \n",
    "        'BldgType': ['1Fam'], \n",
    "        'HouseStyle': ['1Story'],\n",
    "        'BedroomAbvGr': [4],\n",
    "        'HalfBath': [1],\n",
    "        'FullBath': [1],\n",
    "        'LotArea': [10000],\n",
    "        'YearBuilt': [1980],\n",
    "    }\n",
    ")\n",
    "housePrice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9da1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Test Data\n",
    "testhousingData=pd.read_csv('Data/test.csv')\n",
    "testhousingData.head()\n",
    "\n",
    "testhousingData.head()\n",
    "testhousingData_df=testhousingData[['LotArea','BldgType','HouseStyle','YearBuilt','FullBath','HalfBath','BedroomAbvGr']].copy() #,'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67addb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54311a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred1 = saved_linear_clf.predict(testhousingData_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c788b080",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c81f92b",
   "metadata": {},
   "source": [
    "## Test Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a16a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "newark_df=pd.read_csv('Data/Delaware - Newark.csv')\n",
    "newark_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24365127",
   "metadata": {},
   "outputs": [],
   "source": [
    "bear_df=pd.read_csv('Data/Delaware - Bear.csv')\n",
    "bear_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17deaa78",
   "metadata": {},
   "outputs": [],
   "source": [
    "wilmigton_df=pd.read_csv('Data/Delaware - Wilmington.csv')\n",
    "wilmigton_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c329fe9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ames_df=pd.read_csv('Data/IA - Ames.csv')\n",
    "ames_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py_36_env",
   "language": "python",
   "name": "py_36_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
